{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Captioning_Final_Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dewdevil99/Image-Captioning-System/blob/main/Image_Captioning_Final_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT9A06FIYnRs",
        "outputId": "35ff2de7-29ff-4425-b5ee-936af30e6733"
      },
      "source": [
        "import string\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "from pickle import dump, load\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from keras.applications.xception import Xception, preprocess_input\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.merge import add\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHRwiBPXJzvY"
      },
      "source": [
        "# Instructions for importing the dataset from Google Drive\n",
        "\n",
        "Use pd.read_csv('/content/gdrive/My Drive/AI Project Dataset/Flickr8k_text/file_name_as_per_data_flair')\n",
        "\n",
        "Also, /content/gdrive/My Drive/AI Project Dataset/Flickr8k_Dataset contains images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbx78CXMf4by"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDtODoryOUDb"
      },
      "source": [
        "#loading image names and captions from .txt files (from our dataset) and \n",
        "#creating a dictionary consisting of image names as keys and list of captions as values\n",
        "#after that the captions in the description are cleaned (removing '-', punctuation marks, converting to lower case, removing numbers, etc.)\n",
        "\n",
        "# Loading a text file into memory\n",
        "def load_doc(filename):\n",
        "    # Opening the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# get all imgs with their captions\n",
        "def all_img_captions(filename):\n",
        "    file = load_doc(filename)\n",
        "    captions = file.split('\\n')\n",
        "    descriptions ={}\n",
        "    for caption in captions[:-1]:\n",
        "        img, caption = caption.split('\\t')\n",
        "        if img[:-2] not in descriptions:\n",
        "            descriptions[img[:-2]] = [ caption ]\n",
        "        else:\n",
        "            descriptions[img[:-2]].append(caption)\n",
        "    return descriptions\n",
        "\n",
        "#Data cleaning- lower casing, removing puntuations and words containing numbers\n",
        "def cleaning_text(captions):\n",
        "    table = str.maketrans('','',string.punctuation)\n",
        "    for img,caps in captions.items():\n",
        "        for i,img_caption in enumerate(caps):\n",
        "\n",
        "            img_caption.replace(\"-\",\" \")\n",
        "            desc = img_caption.split()\n",
        "\n",
        "            #converts to lowercase\n",
        "            desc = [word.lower() for word in desc]\n",
        "            #remove punctuation from each token\n",
        "            desc = [word.translate(table) for word in desc]\n",
        "            #remove hanging 's and a\n",
        "            desc = [word for word in desc if(len(word)>1)]\n",
        "            #remove tokens with numbers in them\n",
        "            desc = [word for word in desc if(word.isalpha())]\n",
        "            desc1 = []\n",
        "            for word in desc:\n",
        "                x = random.random()\n",
        "                if word == 'man':\n",
        "                    if x <= 0.2:\n",
        "                        desc1.append('person')\n",
        "                    else:\n",
        "                        desc1.append(word)\n",
        "                elif word == 'dog':\n",
        "                    if x <= 0.25:\n",
        "                        desc1.append('pet-dog')\n",
        "                    else:\n",
        "                        desc1.append(word)\n",
        "                elif word == 'two':\n",
        "                    if x <= 0.18:\n",
        "                        desc1.append('couple of')\n",
        "                    else:\n",
        "                        desc1.append(word)\n",
        "                else:\n",
        "                    desc1.append(word)\n",
        "            desc = desc1\n",
        "            #convert back to string\n",
        "\n",
        "            img_caption = ' '.join(desc)\n",
        "            captions[img][i]= img_caption\n",
        "    return captions\n",
        "\n",
        "def text_vocabulary(descriptions):\n",
        "    # build vocabulary of all unique words\n",
        "    vocab = set()\n",
        "\n",
        "    for key in descriptions.keys():\n",
        "        [vocab.update(d.split()) for d in descriptions[key]]\n",
        "\n",
        "    return vocab\n",
        "\n",
        "#All descriptions in one file \n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + '\\t' + desc )\n",
        "    data = \"\\n\".join(lines)\n",
        "    file = open(filename,\"w\")\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "\n",
        "# Set these path according to project folder in you system\n",
        "dataset_text = \"/content/drive/MyDrive/AI Project Dataset/Flickr8k_text\"\n",
        "dataset_images = \"/content/drive/MyDrive/AI Project Dataset/Flickr8k_Dataset\"\n",
        "\n",
        "#we prepare our text data\n",
        "filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
        "#loading the file that contains all data\n",
        "#mapping them into descriptions dictionary img to 5 captions\n",
        "descriptions = all_img_captions(filename)\n",
        "print(\"Length of descriptions =\" ,len(descriptions))\n",
        "\n",
        "#cleaning the descriptions\n",
        "clean_descriptions = cleaning_text(descriptions)\n",
        "\n",
        "#building vocabulary \n",
        "vocabulary = text_vocabulary(clean_descriptions)\n",
        "print(\"Length of vocabulary = \", len(vocabulary))\n",
        "\n",
        "#saving each description to file \n",
        "save_descriptions(clean_descriptions, \"/content/drive/MyDrive/AI dummy folder/descriptions.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gjsV3uDAWfj"
      },
      "source": [
        "#paths to the different files of the dataset\n",
        "dataset_text = \"/content/drive/MyDrive/AI Project Dataset/Flickr8k_text\"\n",
        "dataset_images = \"/content/drive/MyDrive/AI Project Dataset/Flickr8k_Dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcw8UCwAJMAB"
      },
      "source": [
        "#in this cell, we convert all the images in the entire dataset into feature vectors\n",
        "#by doing a forward pass through the Xception network for all the images\n",
        "#these vectors are saved as a pickle file features.p\n",
        "#as this code segment takes a lot of time to run, we run it only once\n",
        "#everytime we want to use feature vectors, we load them through features.p\n",
        "def extract_features(directory):\n",
        "    model = Xception( include_top=False, pooling='avg' )\n",
        "    features = {}\n",
        "    i=0\n",
        "    for img in os.listdir(directory):\n",
        "        i += 1\n",
        "        print(i) \n",
        "        filename = directory + \"/\" + img\n",
        "        image = Image.open(filename)\n",
        "        image = image.resize((299,299))\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        #image = preprocess_input(image)\n",
        "        image = image/127.5\n",
        "        image = image - 1.0\n",
        "\n",
        "        feature = model.predict(image)\n",
        "        # print(type(feature))\n",
        "        # print(\"length\",len(feature))\n",
        "        # print(feature)\n",
        "        # print(feature.shape)\n",
        "        features[img] = feature\n",
        "    return features\n",
        "print(\"Done\")\n",
        "#2048 feature vector\n",
        "features = extract_features(dataset_images)\n",
        "dump(features, open(\"features.p\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoX-VhiRuqeA"
      },
      "source": [
        "#feature vectors loaded through the pickle file\n",
        "features = load(open(\"/content/drive/MyDrive/AI Project Dataset/features.p\",\"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMfWFbciVLH9"
      },
      "source": [
        "#here we load the names of the training images from the dataset\n",
        "#and with the help of that, we create a dictionary consisting of training image names as keys and list of captions as values\n",
        "#in these captions, we have added <start> at the beginning and <end> at the end\n",
        "#after that we create a dictionary of training images and their feature vectors named train_features\n",
        "\n",
        "#load the data \n",
        "def load_photos(filename):\n",
        "    file = load_doc(filename)\n",
        "    photos = file.split(\"\\n\")[:-1]\n",
        "    return photos\n",
        "\n",
        "\n",
        "def load_clean_descriptions(filename, photos): \n",
        "    #loading clean_descriptions\n",
        "    file = load_doc(filename)\n",
        "    descriptions = {}\n",
        "    for line in file.split(\"\\n\"):\n",
        "\n",
        "        words = line.split()\n",
        "        if len(words)<1 :\n",
        "            continue\n",
        "\n",
        "        image, image_caption = words[0], words[1:]\n",
        "\n",
        "        if image in photos:\n",
        "            if image not in descriptions:\n",
        "                descriptions[image] = []\n",
        "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
        "            descriptions[image].append(desc)\n",
        "\n",
        "    return descriptions\n",
        "\n",
        "\n",
        "def load_features(photos):\n",
        "    #loading all features\n",
        "    all_features = load(open(\"/content/drive/MyDrive/AI Project Dataset/features.p\",\"rb\"))\n",
        "    #selecting only needed features\n",
        "    features = {k:all_features[k] for k in photos}\n",
        "    return features\n",
        "\n",
        "\n",
        "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
        "\n",
        "#train = loading_data(filename)\n",
        "train_imgs = load_photos(filename) #list containing training image IDs as strings\n",
        "train_descriptions = load_clean_descriptions(\"/content/drive/MyDrive/AI dummy folder/descriptions.txt\", train_imgs)\n",
        "train_features = load_features(train_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeT0YUPvVPr9"
      },
      "source": [
        "#here we create a tokenizer to vectorise the text corpus (captions), that is,\n",
        "#it converts all the words in the training captions into integers based on their frequency\n",
        "#this is done because we can feed only numbers to our NN\n",
        "\n",
        "#converting dictionary to clean list of descriptions\n",
        "def dict_to_list(descriptions):\n",
        "    all_desc = []\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc\n",
        "\n",
        "#creating tokenizer class \n",
        "#this will vectorise text corpus\n",
        "#each integer will represent token in dictionary\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def create_tokenizer(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(desc_list)\n",
        "    return tokenizer\n",
        "\n",
        "# give each word an index, and store that into tokenizer.p pickle file\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
        "dump(tokenizer, open(\"/content/drive/MyDrive/AI dummy folder/tokenizer.p\",\"wb\"))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsVGwjg-VTHd"
      },
      "source": [
        "#here we calculate the maximum length of the captions\n",
        "\n",
        "\n",
        "#calculate maximum length of descriptions\n",
        "def max_length(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    return max(len(d.split()) for d in desc_list)\n",
        "    \n",
        "max_length = max_length(descriptions)\n",
        "max_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT9_nJXYLCZ0"
      },
      "source": [
        "#data visualization: stop words are removed here\n",
        "\n",
        "tokenizer = load(open(\"/content/drive/MyDrive/AI dummy folder/tokenizer.p\",\"rb\"))\n",
        "\n",
        "\n",
        "def order_by_second_arg(my_tuple):\n",
        "    return my_tuple[1]\n",
        "\n",
        "train_all_words = list(tokenizer.word_counts.keys())\n",
        "train_word_counts = list(tokenizer.word_counts.values())\n",
        "words_and_counts = list(zip(train_all_words, train_word_counts))\n",
        "words_and_counts.sort(reverse=True, key=order_by_second_arg)\n",
        "print(words_and_counts)\n",
        "\n",
        "\n",
        "def remove_stop_words(l, stop_words):\n",
        "    l = [(a,b) for (a,b) in l if a not in stop_words]\n",
        "    return l\n",
        "\n",
        "def unzip(l):\n",
        "    l1 = []\n",
        "    l2 = []\n",
        "    for a,b in l:\n",
        "        l1.append(a)\n",
        "        l2.append(b)\n",
        "    return (l1,l2)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words = list(stop_words)\n",
        "\n",
        "words_and_counts = remove_stop_words(words_and_counts, stop_words)\n",
        "train_all_words, train_word_counts = unzip(words_and_counts)\n",
        "#\"'s\" in train_all_words\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "axes = fig.add_axes([0.1, 0.1, 2.8, 2.8]) # left, bottom, width, height (range 0 to 1)\n",
        "axes.plot(train_all_words[2:20], train_word_counts[2:20], 'b')\n",
        "axes.set_xlabel('Words')\n",
        "axes.set_ylabel('Frequency')\n",
        "axes.set_title('Frequency of words occuring in the descriptions of images in the training set')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd0Jm3wt2oro"
      },
      "source": [
        "#here we create a data generator to generate data one after the other\n",
        "#so that it can be fed into the model during training, as so much data cannot be loaded into the memory at once\n",
        "#here the data generated is of the form: \n",
        "#inputs are partial sentences and the feature vector of the image, outputs are the correct/expected next word\n",
        "#in this way, we teach the model to predict the next word, given a partial sentence\n",
        "\n",
        "\n",
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
        "\t# loop for ever over images\n",
        "\twhile 1:\n",
        "\t\tfor key, desc_list in descriptions.items():\n",
        "\t\t\t# retrieve the photo feature\n",
        "\t\t\tphoto = photos[key][0]\n",
        "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
        "\t\t\tyield ((in_img, in_seq), out_word)\n",
        "\n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each description for the image\n",
        "\tfor desc in desc_list:\n",
        "\t\t# encode the sequence\n",
        "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t# split one sequence into multiple X,y pairs\n",
        "\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t# split into input and output pair\n",
        "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t# pad input sequence\n",
        "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t# encode output sequence\n",
        "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t# store\n",
        "\t\t\tX1.append(photo)\n",
        "\t\t\tX2.append(in_seq)\n",
        "\t\t\ty.append(out_seq)\n",
        "\treturn np.array(X1), np.array(X2), np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jENUUziVVd2M"
      },
      "source": [
        "#Here we define our model. We have a feature vector 2048 input layer followed by a Dense layer.\n",
        "#Then we have an Input layer of the length same as the max length of the captions. Now we combine these two\n",
        "#layers and complete our Deep NN model. The final layer is Dense layer with vocabulary size many classes\n",
        "\n",
        "from keras.utils import plot_model\n",
        "\n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "\n",
        "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    #fe1 = Dropout(0.05)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(inputs1)\n",
        "\n",
        "\n",
        "    # LSTM sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    #se2 = Dropout(0.25)(se1)\n",
        "    se3 = LSTM(256)(se1)\n",
        "\n",
        "    # Merging both models\n",
        "    decoder1 = add([fe2, se3])\n",
        "    \n",
        "    \n",
        "    #decoder4 = Dropout(0.15)(decoder3)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder1)\n",
        "\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ngmgbNR3BqF"
      },
      "source": [
        "#Here we train the model with 30 epochs. It was observed that the model converges to the final accuracy by \n",
        "#epoch 26\n",
        "\n",
        "# train the model, run epochs manually and save after each epoch\n",
        "epochs = 30\n",
        "model = define_model(vocab_size, max_length)\n",
        "steps = len(train_descriptions)\n",
        "#os.mkdir(\"models\")\n",
        "\n",
        "for i in range(epochs):\n",
        "  # create the data generator\n",
        "  generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
        "  # fit for one epoch\n",
        "  model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "# save model\n",
        "  #model.save(\"models/model_8k_3_\" + str(i) + \".h5\")\n",
        "  model.save(\"/content/drive/MyDrive/AI dummy folder/model_8k_3_\" + str(i) + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5BxYnM0kDmk"
      },
      "source": [
        "#This code is to resume training if the traning of the model stopped due to some unforseen (Google-Colab-runtime related) reason\n",
        "\n",
        "epochs=18\n",
        "steps = len(train_descriptions)\n",
        "#os.mkdir(\"models\")\n",
        "#model = define_model(vocab_size, max_length)\n",
        "model = load_model(\"/content/drive/MyDrive/AI dummy folder/model_8k_310.h5\")#\"models/model_9.h5\"\n",
        "for i in range(epochs):\n",
        "  # create the data generator\n",
        "  generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
        "  # fit for one epoch\n",
        "  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "  # save model\n",
        "  #model.save(\"models/model_8k_2\" + str(i+2) + \".h5\")\n",
        "  model.save(\"/content/drive/MyDrive/AI dummy folder/model_8k_3\" + str(i+11) + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "menpE-68VoyC"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import random\n",
        "\n",
        "#We extract the 2048 fature vector in this fucntion\n",
        "def extract_features(filename, model):\n",
        "        try:\n",
        "            image = Image.open(filename)\n",
        "\n",
        "        except:\n",
        "            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n",
        "        image = image.resize((299,299))\n",
        "        image = np.array(image)\n",
        "        # for images that has 4 channels, we convert them into 3 channels\n",
        "        if image.shape[2] == 4: \n",
        "            image = image[..., :3]\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = image/127.5\n",
        "        image = image - 1.0\n",
        "        feature = model.predict(image)\n",
        "        return feature\n",
        "\n",
        "#we get the word corresponding to a particular index from here\n",
        "def word_for_id(integer, tokenizer):\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == integer:\n",
        "         return word\n",
        "  return None\n",
        "\n",
        "#We generate the description here for a given model\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    in_text = 'start'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        pred = model.predict([photo,sequence], verbose=0)\n",
        "        pred = np.argmax(pred)\n",
        "        word = word_for_id(pred, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'end':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "\n",
        "\n",
        "max_length = 38\n",
        "tokenizer = load(open(\"tokenizer.p\",\"rb\"))\n",
        "model = load_model(\"/content/drive/MyDrive/AI dummy folder/model_8k_326.h5\")\n",
        "xception_model = Xception(include_top=False, pooling=\"avg\")\n",
        "i=0\n",
        "#start = random.randint(1,2000)\n",
        "start = 1145\n",
        "print(start)\n",
        "for im in os.listdir(dataset_images + \"/\"):\n",
        "  if im not in train_imgs:\n",
        "    if i >=start:\n",
        "      photo = extract_features(dataset_images + \"/\" + im, xception_model)\n",
        "      img = Image.open(dataset_images + \"/\" + im)\n",
        "      captions = descriptions[im]\n",
        "\n",
        "      description = generate_desc(model, tokenizer, photo, max_length)\n",
        "      hypo = description.split()\n",
        "      ref = []\n",
        "      for c in captions:\n",
        "        caption = c.split()\n",
        "        ref.append(caption)\n",
        "\n",
        "      bleuScore_bi = nltk.translate.bleu_score.sentence_bleu(ref,hypo,weights=(0,1,0,0))\n",
        "      bleuScore_uni = nltk.translate.bleu_score.sentence_bleu(ref,hypo,weights=(1,0,0,0))\n",
        "      bleuScore_cum = nltk.translate.bleu_score.sentence_bleu(ref,hypo,weights=(0.25,0.25,0.25,0.25))\n",
        "      print(\"\\n\\n\")\n",
        "      print(description)\n",
        "      print(\"\\n\")\n",
        "      print(\"BLEU Score Uni = \",bleuScore_uni)\n",
        "      print(\"BLEU Score Bi = \",bleuScore_bi)\n",
        "      print(\"BLEU Score Cum= \",bleuScore_cum)\n",
        "      plt.imshow(img)\n",
        "      plt.pause(5)\n",
        "    if i>start + 10:\n",
        "      break\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf6DLlyt126v"
      },
      "source": [
        "#Code to predict captions on a video\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "def extract_features(frame, model):\n",
        "        image=cv2.resize(frame,(299,299),fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "        image = np.array(image)\n",
        "        #for images that has 4 channels, we convert them into 3 channels\n",
        "        if image.shape[2] == 4: \n",
        "            image = image[..., :3]\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = image/127.5\n",
        "        image = image - 1.0\n",
        "        feature = model.predict(image)\n",
        "        return feature\n",
        "\n",
        "def word_for_id(integer, tokenizer):\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == integer:\n",
        "         return word\n",
        "  return None\n",
        "\n",
        "\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    in_text = 'start'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        pred = model.predict([photo,sequence], verbose=0)\n",
        "        pred = np.argmax(pred)\n",
        "        word = word_for_id(pred, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'end':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "# Same command function as streaming, its just now we pass in the file path, nice!\n",
        "cap = cv2.VideoCapture('/content/drive/MyDrive/AI dummy folder/video.mov')\n",
        "\n",
        "\n",
        "# Always a good idea to check if the video was acutally there\n",
        "# If you get an error at thsi step, triple check your file path!!\n",
        "if cap.isOpened()== False: \n",
        "    print(\"Error opening the video file. Please double check your file path for typos. Or move the movie file to the same location as this script/notebook\")\n",
        "    \n",
        "frame_seq=0\n",
        "time_length = 11\n",
        "fps=24\n",
        "# While the video is opened\n",
        "while cap.isOpened():\n",
        "    \n",
        "    \n",
        "    #print(\"Inside while\")\n",
        "    frame_no = (frame_seq /(time_length*fps))\n",
        "    cap.set(1,frame_seq)\n",
        "    # Read the video file.\n",
        "    ret, frame = cap.read()\n",
        "    \n",
        "    # If we got frames, show them.\n",
        "    if ret == True:\n",
        "        photo = extract_features(frame, xception_model)\n",
        "        \n",
        "        description = generate_desc(model, tokenizer, photo, max_length)\n",
        "        print(\"\\n\\n\")\n",
        "        print(description)\n",
        "        frame=cv2.resize(frame,(450,300),fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "        cv2_imshow(frame)\n",
        "        #plt.imshow(frame)\n",
        "        frame_seq+=48\n",
        "\n",
        "        # Press q to quit\n",
        "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "            \n",
        "            break\n",
        "        plt.pause(5) \n",
        "    # Or automatically break this whole loop if the video is over.\n",
        "    else:\n",
        "        print(\"Breaking loop\")\n",
        "        break\n",
        "        \n",
        "cap.release()\n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_olpp-aR00P8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}